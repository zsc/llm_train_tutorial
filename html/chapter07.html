<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <base href="./">
    <title>第 7 章 — 优化器与数值稳定</title>
    <link rel="stylesheet" href="assets/style.css">
    <link rel="stylesheet" href="assets/highlight.css">
    <script src="assets/script.js" defer></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$']],
                displayMath: [['$$', '$$']],
                processEscapes: false,
                packages: {'[+]': ['noerrors', 'ams']}
            },
            options: {
                ignoreHtmlClass: 'tex2jax_ignore',
                processHtmlClass: 'tex2jax_process'
            },
            loader: {
                load: ['[tex]/noerrors', '[tex]/ams']
            }
        };
    </script>
</head>
<body>
    <div class="container">
        <nav id="sidebar" class="sidebar">
            <div class="sidebar-header">
                <h3>目录</h3>
                <button id="sidebar-toggle" class="sidebar-toggle">
                    <span></span>
                    <span></span>
                    <span></span>
                </button>
            </div>
            <div class="sidebar-search">
                <input type="text" id="sidebar-search-input" placeholder="搜索..." autocomplete="off">
            </div>
            <div id="tree-container">
                <nav class="tree-nav" role="tree">
                    <div class="tree-item " >
                        <a href="index.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">从零到可复现：LLM 训练实战（算法向，Lightning + DeepSpeed）—**索引**</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter1.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">chapter01.md — 总览与可复现环境</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter2.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Untitled</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter3.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">chapter03.md — 架构细节：LLaMA 风格与 8k 扩展</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter4.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Untitled</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter05.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Chapter 05 — 大批量训练与学习率策略</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter06.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第六章：多数据集动态混比——从“大锅饭”到“交响乐”</span>
                        </a>
                    </div>
                
                    <div class="tree-item active" >
                        <a href="chapter07.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 7 章 — 优化器与数值稳定</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter08.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 8 章：数据加载与存储格式（CPFS）— 榨干 IO 吞吐</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter09.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第九章 — 并行与内存：Lightning + DeepSpeed 配方</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter10.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">chapter10.md — 评估：验证困惑度</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter11.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">chapter11.md — 端到端：从零预训练（1T tokens）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter12.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第十二章 端到端：CPT / 继续预训练</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter13.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 13 章：成本/时长粗估（¥）与 TCO</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter14.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">chapter14.md — 常见问题与诊断</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter15.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">chapter15.md — 附录与参考</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="CLAUDE.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Untitled</span>
                        </a>
                    </div>
                </nav>
            </div>
        </nav>
        
        <main class="content">
            <article>
                <h1 id="7">第 7 章 — 优化器与数值稳定</h1>
<h2 id="_1">开篇段落</h2>
<p>如果说模型架构是训练的“骨架”，数据是“血液”，那么优化器与数值稳定性策略就是整个训练过程的“心脏与循环系统”。一个微小的超参失当，或是一次未被察觉的数值溢出，都可能导致数千 GPU 小时的努力付诸东流。本章，我们将深入这间至关重要的“引擎室”，对 LLM 训练中的核心组件进行精细解剖。我们将从默认王者 <strong>AdamW</strong> 出发，不仅探讨其超参的“what”，更深究其“why”，特别是 <code>β₂</code> 和解耦权重衰减的现代选择。随后，我们将审视 <strong>Lion</strong>、<strong>Adafactor</strong> 等备选方案的独特设计与适用界，并详解 <strong>DeepSpeed Paged AdamW</strong> 如何打破显存壁垒。更重要的是，我们将系统性地构建一套数值稳定性的“纵深防御体系”，涵盖<strong>梯度累积</strong>、<strong><code>bf16</code> 混合精度</strong>的奥秘、<strong>损失缩放</strong>的动态机制以及<strong>梯度裁剪</strong>的必要性。学完本章，你将不仅掌握一套“能用”的配置，更能理解其背后的原理，从而在面对训练不稳时，具备庖丁解牛般的诊断与调试能力。</p>
<h2 id="_2">文字论述</h2>
<h3 id="1-adamw">1. AdamW: 精调默认的王者</h3>
<p>AdamW (Adam with Decoupled Weight Decay) 凭借其鲁棒性和高效性，已成为大规模语言模型训练的黄金标准。然而，“默认”不代表“无需理解”。恰恰相反，对其关键超参的精细调整是通往 SOTA 性能的第一步。</p>
<h4 id="11-decoupled-weight-decay">1.1 解耦权重衰减 (Decoupled Weight Decay) 的威力</h4>
<p>要理解 AdamW，首先要明白它与 Adam + L2 正则化的区别。</p>
<ul>
<li>
<p><strong>传统 L2 正则化</strong>: 在损失函数中加入 <code>(λ/2) * ||w||²</code> 项。这导致梯 <code>g_t</code> 变为 <code>g_t' = g_t + λ * w_t</code>。这个“正则化梯度”随后被送入 Adam 的动量和二阶矩估计中，影响了自适应学习率的计算。对于梯度较大（通常是信息量大）的权重，<code>λ * w_t</code> 的影响被 <code>sqrt(v_t)</code> 项削弱；对于梯度较小的权重，其影响反而被放大。这并非我们想要的正则化效果。</p>
</li>
<li>
<p><strong>解耦权重衰减</strong>: AdamW 将权重衰减直接从梯度更新中分离出来。在计算完基于梯度的更新步长 <code>Δw_t</code> 后，直接从权重中减去一个与其自身大小成正比的量。</p>
<ul>
<li>更新公式（概念上）：<code>w_{t+1} = w_t - η * Δw_t - η * λ * w_t</code></li>
<li>这意味着权重衰减的效果与学习率 <code>η</code> 和衰减系数 <code>λ</code> 直接相关，且独立于梯度的历史信息。它更像是一种朝向原点的“匀速”收缩，是更纯粹、更可控的正则化。</li>
</ul>
</li>
</ul>
<p><strong>经验法则</strong>: 对于 LLM 预训练，权重衰减系数 <code>wd</code> (即 <code>λ</code>) 的一个非常稳健的起始点是 <strong><code>0.1</code></strong>。</p>
<h4 id="12">1.2 键超参的现代理解</h4>
<ul>
<li>
<p><strong><code>β₁</code> (一阶矩衰减率)</strong>: <code>0.9</code> 几乎是所有任务的通用标准，它控制了动量的“记忆长度”。修改它的风险高，收益不确定，因此我们通常固定此值。</p>
</li>
<li>
<p><strong><code>β₂</code> (二阶矩衰减率)</strong>: <strong>这是 LLM 训练中最关键的超参之一</strong>。</p>
<ul>
<li><strong>传统值 <code>0.999</code></strong>: 意味着优化器对过去 1000 步的梯度平方信息有很长的“记忆”。这在相对平滑的损失曲面上表现良好。但在 LLM 训练这种高度非凸、梯度充满噪声的环境中，过长的记忆就像一艘笨重的油轮，转向缓慢，无法快速适应新的梯度信息。</li>
<li><strong>现代值 <code>0.95</code></strong>: LLaMA 等工作的实践表明，更短的记忆（约 20 步）能让优化器像一艘灵活的快艇。当遇到一小段损失平稳期（plateau）或梯度分布突然变化时，<code>v_t</code> 能更快地“忘记”旧信息，采纳新信息，从而调整每个参数的学习率，帮助模型更快地逃离困境。<strong>对于从零训练，<code>β₂=0.95</code> 是强烈推荐的默认值。</strong></li>
</ul>
</li>
<li>
<p><strong><code>ε</code> (Epsilon)</strong>: 分母稳定项。</p>
<ul>
<li>其值应与数值精度相匹配。在 <code>fp32</code> 下，<code>1e-8</code> 是标准值。但在 <code>bf16/fp16</code> 混合精度下，由于数值表示范围变窄，一个稍大的 <code>ε</code> 如 <strong><code>1e-6</code> 或 <code>1e-5</code></strong> 能提供额外的保护，防止 <code>sqrt(v_t)</code> 接近零时导致分母过小，更新步长爆炸。</li>
</ul>
</li>
</ul>
<h4 id="13-fused-adamw">1.3 Fused AdamW: 榨干硬件性能</h4>
<p>在 H100 规模的训练中，每一步的耗时都至关重要。优化器步骤（<code>optimizer.step</code>）可能成为一个不可忽视的瓶颈，因为它涉及大量对参数和优化器状态的读写操作。</p>
<ul>
<li><strong>瓶颈分析</strong>: 这个过程是<strong>内存带宽密集型 (Memory-Bandwidth Bound)</strong> 而非计算密集型。标准的 PyTorch 实现会为每个操作（加法、乘法、开方等）启动一个独立的 CUDA kernel。频繁的 kernel 启动开销和全局内存的反复读写会严重拖慢执行速度。</li>
<li><strong>融合 (Fusing)</strong>: Fused AdamW 将整个更新逻辑（包括梯度缩放、动量计算、二阶矩更新、权重衰减和最终的参数更新）合并成一个或少数几个高性能的 CUDA kernel。这使得所有计算尽可能在 GPU 的寄存器和 L1/L2 缓存中完成，一次性从全局内存（HBM）读取数据，计算完毕后再写回，<strong>极大减少了内存I/O，从而将优化器步骤提速数倍</strong>。DeepSpeed 和 Apex 都提供了此类实现，在我们的技术栈中应默认启用。</li>
</ul>
<h3 id="2">2. 备选方案：特定场景下的利器</h3>
<ul>
<li>
<p><strong>Lion (EvoLved Sign Momentum)</strong>:</p>
<ul>
<li><strong>核心机制</strong>: <code>update = sign(β₁*m_{t-1} + (1-β₁)g_t)</code>。它完全抛弃了二阶矩 <code>v_t</code>，更新方向仅由动量的符号决定，更新步长则由全局学习率 <code>η</code> 控制。</li>
<li><strong>优劣分析</strong>:<ul>
<li><strong>优点</strong>: 内存占用减半（只需存 <code>m_t</code>），在某些基准测试中展现出比 AdamW 更强的性能。其恒定的更新幅度可能有助于跨越一些尖锐的局部最小值。</li>
<li><strong>缺点</strong>: 超参其敏感。学习率 <code>η</code> 通常需要设为 AdamW 的 <code>1/5</code> 到 <code>1/3</code>，而 <code>wd</code> 也需要相应放大 5-10 倍。<code>β₁</code> 和 <code>β₂</code> 也需要重新调整。它是一种高风险高回报的选择，不适合作为初次尝试的基线。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Adafactor</strong>:</p>
<ul>
<li><strong>核心机制</strong>: 为了节省内存，它不存储完整的二阶矩 <code>v_t</code>，而是将其近似为一个低秩分解矩阵。它也不存储一阶动量，除非 <code>β₁</code> 不为零。</li>
<li><strong>历史地位与现状</strong>: 在 ZeRO 技术出现之前，Adafactor 是训练超大模型（如 T5）的唯一选择。但在 DeepSpeed ZeRO 普及的今天，其近似带来的潜在性能损失，使其吸引力大大下降。在我们的 H100 集群上，有 Paged AdamW 和 ZeRO-3，几乎没有理由再选择 Adafactor。</li>
</ul>
</li>
<li>
<p><strong>DeepSpeed Paged AdamW: 内存的无限游戏</strong></p>
<ul>
<li><strong>工作原理</strong>: 它将 CPU 的系统内存（DRAM）视为 GPU 高速缓存（HBM）的扩展。绝大多数优化器状态（FP32 权重副本、动量、方差）都存放在 CPU 内存中。当需要为某一层参数更新时，DeepSpeed 会<strong>异步地、提前地</strong>将这些状态从 CPU “分页(page in)”到 GPU 的一块预留缓存区中。计算完成后，更新后的状态再被“分页(page out)”回 CPU 内存。</li>
<li><strong>ASCII 流程图</strong>:</li>
</ul>
</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="c">CPU DRAM (Hundreds of GBs)             GPU HBM (80GB)</span>
<span class="nb">+----------------------------+</span><span class="c">         </span><span class="nb">+-----------------+</span>
<span class="c">| Optimizer States (Layer N) | </span><span class="nb">------</span><span class="nv">&gt;</span><span class="c"> | Paging Buffer   | </span><span class="nb">--</span><span class="c">(compute)</span><span class="nb">--</span><span class="nv">&gt;</span><span class="c"> Updated States</span>
<span class="c">| Optimizer States (Layer N</span><span class="nb">+</span><span class="c">1)|         </span><span class="nb">+-----------------+</span><span class="c">       |</span>
<span class="c">| </span><span class="nt">...</span><span class="c">                        | </span><span class="nv">&lt;</span><span class="nb">------</span><span class="c"> (write back) </span><span class="nv">&lt;</span><span class="nb">-------------+</span>
<span class="nb">+----------------------------+</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="k">*</span>   **关键**: **异步预取 (Asynchronous Prefetching)** 是隐藏延迟的关键。在 GPU 计算第 N 层的反向传播时，DeepSpeed 的后台线程已经在通过 PCIe 总线拉取第 N+1 层的优化器状态了。
<span class="k">*</span>   **适用性**: 对于 13B 及以上规模的模，即便有 80GB HBM，优化器状态依然会占据巨大空间。Paged AdamW 配合 ZeRO-3 是解锁更大模型、更大批次训练能力的**核心技术**。
</code></pre></div>

<h3 id="3">3. 数值稳定性的纵深防御体系</h3>
<h4 id="31-gradient-accumulation-a">3.1 梯度累积 (Gradient Accumulation, <code>A</code>)</h4>
<p>梯度累积是在不牺牲内存的前提下，达成大批量训练目标的基础。
<code>Global batch size (tokens) = micro_batch_size * num_gpus (D) * gradient_accumulation_steps (A)</code>
从优化器的视角看，它在每 <code>A</code> 个 micro-step 之后，看到的是一个由 <code>D * A</code> 个 micro-batch 累加而成的梯度，这等效于用一个巨大的 batch size 进行了一次更新。这对于稳定训练、利用 large-batch scaling law 至关重要。</p>
<h4 id="32-bf16">3.2 混合精度训练：<code>bf16</code> 的统治</h4>
<p>选择正确的数值格式是速度与稳定性的核心权衡。</p>
<p>| 格式  | 符号位 | 指数位 | 尾数位 | 动态范围 (近似)      | 精度           | H100 Tensor Core 支持 | 备注                                   |</p>
<table>
<thead>
<tr>
<th style="text-align: left;">格式</th>
<th style="text-align: left;">符号位</th>
<th style="text-align: left;">指数位</th>
<th style="text-align: left;">尾数位</th>
<th style="text-align: left;">动态范围 (近似)</th>
<th style="text-align: left;">精度</th>
<th style="text-align: left;">H100 Tensor Core 支持</th>
<th style="text-align: left;">备注</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">FP32</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">8</td>
<td style="text-align: left;">23</td>
<td style="text-align: left;"><code>10⁻³⁸</code> to <code>10³⁸</code></td>
<td style="text-align: left;">高 (基线)</td>
<td style="text-align: left;">(通过 TF32 模拟)</td>
<td style="text-align: left;">稳定但慢，内存占用大</td>
</tr>
<tr>
<td style="text-align: left;">TF32</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">8</td>
<td style="text-align: left;">10</td>
<td style="text-align: left;"><code>10⁻³⁸</code> to <code>10³⁸</code></td>
<td style="text-align: left;">中等</td>
<td style="text-align: left;"><strong>是</strong></td>
<td style="text-align: left;">Hopper/Ampere 默认 Matmul 格式，对用户透明</td>
</tr>
<tr>
<td style="text-align: left;"><strong>BF16</strong></td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">8</td>
<td style="text-align: left;">7</td>
<td style="text-align: left;"><strong><code>10⁻³⁸</code> to <code>10³⁸</code></strong></td>
<td style="text-align: left;">低</td>
<td style="text-align: left;"><strong>是</strong></td>
<td style="text-align: left;"><strong>LLM 训练首选</strong>，动态范围同 FP32</td>
</tr>
<tr>
<td style="text-align: left;">FP16</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">5</td>
<td style="text-align: left;">10</td>
<td style="text-align: left;"><code>6x10⁻⁸</code> to <code>65504</code></td>
<td style="text-align: left;">中等</td>
<td style="text-align: left;">是</td>
<td style="text-align: left;">动态范围窄，极易溢出，需强依赖损失缩放</td>
</tr>
</tbody>
</table>
<p><strong>结论</strong>: <code>bf16</code> 的 8 位指数位使其拥有与 <code>fp32</code> 相同的动态范围，这意味着它能表示极大和极小的数值，从根本上解决了 <code>fp16</code> 的上溢（overflow）和下溢（underflow）问题。虽然其 7 位尾数牺牲了精度，但大量研究表明，对于神经网络随机梯度下降的本质来说，这种精度损失几乎不影响最终收敛效果。<strong>在 H100 平台上，<code>bf16</code> 是毫无疑问的最佳选择</strong>。</p>
<h4 id="33-dynamic-loss-scaling">3.3 动态损失缩放 (Dynamic Loss Scaling)</h4>
<p>即便在使用 <code>bf16</code> 时，损失缩放依然是一道重要的、几乎零成本的保险。</p>
<ul>
<li><strong>机制详解</strong>:<ol>
<li><strong>初始化</strong>: 设置一个初始缩放因子 <code>S_init</code> (e.g., 2^16) 和一个检查周期 <code>N_growth</code> (e.g., 1000 steps)。</li>
<li><strong>缩放</strong>: <code>loss_scaled = loss * S</code>。</li>
<li><strong>反向传播</strong>: <code>loss_scaled.backward()</code>。梯度也被放大了 <code>S</code> 倍。</li>
<li><strong>梯度反缩放与检查</strong>: 在 <code>optimizer.step()</code> 之前，框架会遍历所有梯度，检查是否存在 <code>Inf</code> 或 <code>NaN</code>。<ul>
<li><strong>若存在溢出</strong>: 跳过此次权重更新。将 <code>S</code> 除以 2（<code>S /= 2</code>），重置检查周期计数器。</li>
<li><strong>若无溢出</strong>: 将所有梯度除以 <code>S</code> 恢复原值，然后执行 <code>optimizer.step()</code>。检查周期计数器加一。如果计数器达到 <code>N_growth</code>，则将 <code>S</code> 乘以 2（<code>S *= 2</code>），以尝试保留更小的梯度细节。</li>
</ul>
</li>
</ol>
</li>
<li><strong>作用</strong>: 这个动态调节机制确保了 <code>S</code> 始终处在一个“既能防止下溢，又不会导致上溢”的甜点区。在 PyTorch Lightning/DeepSpeed 中，只需开启相应配置，整个过程完全自动化。</li>
</ul>
<h4 id="34-gradient-clipping">3.4 梯度裁剪 (Gradient Clipping)</h4>
<p>在训练初期，或者当遇到不稳定的数据批次时，梯度可能会发生爆炸，导致更新步长过大，瞬间摧毁模型的已有学习成果。梯度裁剪是限制这一破坏的“安全阀”。</p>
<ul>
<li><strong>机制</strong>: 在优化器更新前，计算所有模型参数梯度的 L2 范数（<code>grad_norm</code>）。<ul>
<li><code>grad_norm = sqrt(sum(g_i² for g in all_gradients))</code></li>
</ul>
</li>
<li><strong>裁剪</strong>: 如果 <code>grad_norm</code> 超过一个预设的阈值 <code>max_norm</code>，则对所有梯度进行缩放：<ul>
<li><code>g_clipped = g * (max_norm / grad_norm)</code></li>
</ul>
</li>
<li>这保证了梯度的“方向”不变，但其“度”被限制在 <code>max_norm</code> 以内。</li>
<li><strong>经验法则</strong>: 对于 LLM 训练，一个常见的 <code>max_norm</code> 值是 <strong><code>1.0</code></strong>。在训练日志中监控 <code>grad_norm</code> 的实际值非常重要。如果它频繁触及裁剪阈值，可能意味着学习率过高或存在其他不稳定性因素。</li>
</ul>
<h2 id="_3">本章小结</h2>
<ul>
<li><strong>优化器基线</strong>: <strong>AdamW</strong> 是最可靠的选择。使用 <strong>Fused</strong> 实现，并设置现代超参：<code>β₁=0.9</code>, <strong><code>β₂=0.95</code></strong>, <code>ε=1e-6</code>, <code>wd=0.1</code>。</li>
<li><strong>内存管理</strong>: 面对 13B+ 模型，<strong>DeepSpeed Paged AdamW</strong> 结合 ZeRO-3 是打破显存墙、实现大规模训练的关键。</li>
<li><strong>数值精度</strong>: <strong><code>bf16</code> 是 H100 平台的标准配置</strong>。它提供了与 <code>fp32</code> 相同的动态范围，极大地提升了训练稳定性，同时带来显著的性能和内存优势。</li>
<li><strong>稳定“铁三角”</strong>:<ol>
<li><strong>梯度累积</strong>: 实现大批量训练的基石，用以满足 Scaling Law 对 <code>GB_tok</code> 的要求。</li>
<li><strong>动态损失缩放</strong>: 混合精度训练的自动化“保丝”，防止梯度因数值范围问题而丢失或爆炸。</li>
<li><strong>梯度裁剪</strong>: 限制梯度爆炸的“安全阀”，<code>max_norm=1.0</code> 是一个稳健的起点。</li>
</ol>
</li>
</ul>
<h2 id="gotchas">常见陷阱与错误 (Gotchas)</h2>
<ol>
<li>
<p><strong><code>β₂</code> 沿用旧值 <code>0.999</code></strong>:</p>
<ul>
<li><strong>症状</strong>: 模型训练初期 loss 下降极其缓慢，或者在训练中期轻易地陷入一个平稳期（plateau）无法自拔。</li>
<li><strong>诊断</strong>: 将 <code>β₂</code> 想象成优化器的“惯性”。<code>0.999</code> 的惯性巨大，难以转向。<code>0.95</code> 则灵活得多，能快速响应梯度变化。</li>
<li><strong>补救</strong>: 如果训练已经开始，可以考虑在 loss 平稳期“热切换”<code>β₂</code> 到 <code>0.95</code>，有时能激活训练。但最好的做法是从一开始就使用 <code>0.95</code>。</li>
</ul>
</li>
<li>
<p><strong>Lion/其他新优化器与学习率不匹配</strong>:</p>
<ul>
<li><strong>症状</strong>: 切换到 Lion 优化器后，训练在第一个 step 就 <code>loss=NaN</code>。</li>
<li><strong>诊断</strong>: Lion 的更新机制对学习率的尺度要求与 AdamW 完全不同。直接沿用 AdamW 的 LR schedule 是灾难性的。</li>
<li><strong>补救</strong>: 严格遵循 Lion 论文或官方实现的建议，将 AdamW 的峰值学习率除以 3-10 作为 Lion 的起始点，并对权重衰减进行相应放大。对任何新优化器，都要假定其超参空间不兼容，必须从小规模实验开始重新搜索。</li>
</ul>
</li>
<li>
<p><strong>梯度范数持续触顶或为 <code>Inf</code>/<code>NaN</code></strong>:</p>
<ul>
<li><strong>症状</strong>: 日志中 <code>grad_norm</code> 持续等于 <code>max_norm</code> (e.g., 1.0)，或者直接报告 <code>grad_norm</code> 是 <code>Inf</code>。损失缩放因子 <code>S</code> 不断下降。</li>
<li><strong>诊断 checklist</strong>:<ol>
<li><strong>学习率过高?</strong>: 这是最常见的原因。尝试降低峰值学习率 30%-50%。</li>
<li><strong>数据问题?</strong>: 检查当前批次的数据，是否存在异常值或损坏的样本。</li>
<li><strong>模型模块不稳定?</strong>: 某个自定义的算子或数值计算在 <code>bf16</code> 下不稳定？可以尝试用 PyTorch hooks 打印出每一层的梯度范数，定位到第一个出现 <code>Inf</code> 的层。</li>
<li><strong>初始化问题?</strong>: 检查权重初始化否合理，不当的初始化可能导致早期激活值或梯度爆炸。</li>
<li><strong>RoPE Scaling 伪影?</strong>: 在使用 RoPE scaling 扩展上下文时，不当的 <code>base</code> 或 <code>theta</code> 值也可能引入数值问题。</li>
</ol>
</li>
</ul>
</li>
<li>
<p><strong>忽视 Paged AdamW 的 I/O 瓶颈</strong>:</p>
<ul>
<li><strong>症状</strong>: 开启 Paged AdamW 后，GPU 利用率显著下降，<code>tokens/s</code> 不升反降。</li>
<li><strong>诊断</strong>: 使用 <code>nvidia-smi dmon</code> 或 <code>dcgm</code> 监控 PCIe 带宽使用率。如果带宽持续被打满，说明 CPU-GPU 的数据拷贝成了瓶颈。</li>
<li><strong>补救</strong>:<ul>
<li>确认使用的是高速的 PCIe Gen4/Gen5 连接。</li>
<li>在多 NUMA 节点的服务器上，确保训练进程和其使用的 GPU 绑定在同一个 NUMA 节点上，避免跨节点内存访问带来的高延迟。</li>
<li>增加 DeepSpeed 配置中用于 pining 和 prefetching 的 buffer 大小，但这会消耗更多 CPU 内存。</li>
<li>最终，如果硬件瓶颈无法解决，可能需要接受较低的吞吐，或者调整并行策以减少需要 offload 的数据量。</li>
</ul>
</li>
</ul>
</li>
</ol>
            </article>
            
            <nav class="page-nav"><a href="chapter06.html" class="nav-link prev">← 第六章：多数据集动态混比——从“大锅饭”到“交响乐”</a><a href="chapter08.html" class="nav-link next">第 8 章：数据加载与存储格式（CPFS）— 榨干 IO 吞吐 →</a></nav>
        </main>
    </div>
</body>
</html>