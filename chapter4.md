## chapter04.md — Scaling Laws 深入：计算、数据与性能的权衡艺术

### 1. 开篇段落

本章是大型语言模型训练的理论核心与战略指南，我们将深入探讨指导整个训练过程的“物理定律”——**缩放定律（Scaling Laws）**。这不仅是一组数学公式，更是我们在资源有限的现实世界中，进行关键权衡的决策框架。本章将系统性地回答所有 LLM 训练项目启动前最核心的问题：如果我们拥有一笔固定的计算预算（例如，64块H100运行一个月），应该如何将其最优地分配给**模型参数量（`N`）**和**训练数据量（`T`）**？这个决策将如何决定性地影响模型的最终性能？我们将从 Kaplan 等人的开创性工作讲起，重点剖析具有里程碑意义的 **Chinchilla-style** 计算最优理论，并给出其详细推导。随后，我们将引入**噪声尺度定律**，揭示大批量训练的内在动力学，并解释其如何约束学习率和批次大小的选择。最后，本章将结合 2024 年的最新研究进展和业界共识，对 Chinchilla 定律进行刷新和补充，为我们采用 1T token 预算训练 3B/7B/13B 模型的策略选择，提供坚实、前沿且可落地的理论依据。

### 2. 文字论述

#### 2.1 从 Kaplan 到 Chinchilla-style：计算最优之路的范式转移

在 LLM 领域的“寒武纪大爆发”之前，主流观点深受 Kaplan 等人（OpenAI, 2020）研究的影响，其核心结论可以通俗地概括为“模型越大，性能越好，数据的作用相对次要”。他们通过实验发现，在固定的训练步数下，模型的测试损失 `L` 会随着模型参数 `N`、数据集大小 `D` 和计算量 `C` 的增加而呈现可预测的幂律下降。

其损失函数模型大致可以表达为三个独立部分的贡献：
$$
L(N, D, C) = L_N(N) + L_D(D) + L_C(C)
$$
其中，模型大小 `N` 的贡献项为 $L_N(N) = (\frac{N_c}{N})^{\alpha_N}$，数据大小 `D` 的贡献项为 $L_D(D) = (\frac{D_c}{D})^{\alpha_D}$。他们的研究表明，损失对模型大小 `N` 的依赖性（即幂指数 $\alpha_N$）远大于对数据大小的依赖性。这导致业界在一段时间内陷入了对模型规模的“军备竞赛”，认为只要将模型做得足够大，性能就能持续提升。

然而，这一范式在 2022 年被 DeepMind 的 Hoffmann 等人发表的 Chinchilla 论文彻底颠覆。他们敏锐地指出，之前的研究普遍处于一个“计算受限”而非“数据受限”的状态，即模型还没有被数据“喂饱”，因此扩大模型规模带来的收益显得尤为突出。通过在更广阔的参数空间（从 70M 到 16B 模型，从 5B 到 500B token）进行超过 400 次训练实验，他们得出了一个惊人的结论：为了在给定的计算预算 `C`（以 FLOPs 计）下达到最低的损失，**模型参数量 `N` 和训练 token 数 `T` 应该以近乎等比例的方式进行缩放**。

**Chinchilla-style 最优解的详细推导：**

1.  **定义计算预算 (Compute Budget)**：我们首先明确，训练一个 decoder-only 模型的总计算量 `C` (FLOPs) 主要由前向和后向传播决定，可以被精确地近似为：
    $$
    C \approx 6 \cdot N_{\text{params}} \cdot T_{\text{tokens}}
    $$
    这里的 `6` 是一个经验常数，`2` 来自于每个参数在一次前向传播中的一次乘法和一次加法（`2NT`），而反向传播的计算量约是前向传播的两倍（`4NT`），合计 `6NT`。这个公式构成了我们优化的核心**约束条件**。

2.  **建立损失模型 (Loss Model)**：Chinchilla 团队发现，最终的交叉熵损失 `L` 可以被一个三参数模型精准地拟合。该模型将损失 `L` 表达为模型非嵌入参数 `N` 和训练 tokens `T` 的函数：
    $$
    L(N, T) = E_{\infty} + \frac{A}{N^{\alpha}} + \frac{B}{T^{\beta}}
    $$
    *   $E_{\infty}$：代表**不可约损失**，这是数据分布本身固有的熵所决定的理论下限。即使拥有无限大的模型和无限多的数据，也无法将损失降到 $E_{\infty}$ 以下。
    *   $\frac{A}{N^{\alpha}}$：**模型容量误差项**。随着模型参数 `N` 增加，这一项减小，代表模型表达能力的增强。
    *   $\frac{B}{T^{\beta}}$：**数据拟合误差项**。随着训练数据 `T` 增加，这一项减小，代表模型从数据中学到了更多模式。
    *   `A, B, α, β` 是通过大量实验数据拟合得到的正常数。在 Chinchilla 论文中，拟合值为 `α ≈ 0.34`, `β ≈ 0.28`。

3.  **求解约束最优化问题**：我们的目标是在约束 `C = 6NT` 的前提下，最小化 `L(N, T)`。这是一个典型的拉格朗日乘子法问题，但更直观的方法是变量替换。从约束中解出 `T = C / (6N)`，并代入损失函数，将其转化为关于 `N` 的单变量数 `L(N)`：
    $$
    L(N) = E_{\infty} + A \cdot N^{-\alpha} + B \cdot \left(\frac{C}{6N}\right)^{-\beta} = E_{\infty} + A \cdot N^{-\alpha} + B \cdot (C/6)^{-\beta} \cdot N^{\beta}
    $$
    为了找到使损失最小的 `N`，我们对 `N` 求导并令其为零：
    $$
    \frac{dL}{dN} = -A\alpha N^{-\alpha-1} + B\beta (C/6)^{-\beta} N^{\beta-1} = 0
    $$
    移项整理后得到：
    $$
    A\alpha N^{-\alpha-1} = B\beta (C/6)^{-\beta} N^{\beta-1}
    $$
    将 $T^{-\beta} = (C/6N)^{-\beta}$ 代回，可以得到一个极其深刻的直观关系：
    $$
    \alpha \cdot \frac{A}{N^{\alpha}} \approx \beta \cdot \frac{B}{T^{\beta}}
    $$
    这个公式的含义是：在计算最优的训练点上，**由模型规模带来的边际损失降低，应该与由数据规模带来的边际损失降低相平衡**。换句话说，你不应该让模型成为瓶颈，也不应该让数据成为瓶颈。

**Rule-of-thumb (Chinchilla):**
> 对于给定的计算预算，要达到最低的预训练失，训练的 token 数量应约为模型非嵌入参数量的 **20 倍**。
> $$ T_{\text{optimal}} \approx 20 \times N_{\text{params}} $$

**应用到我们的场景 (1T tokens 预算):**
下表清晰地展示了我们的训练计划与 Chinchilla 计算最优建议的对比：

| 模型规模 (`N`) | Chinchilla 推荐 `T` (计算最优) | 我们的训练 `T` | 结论与策略                                     |
| :------------- | :--------------------------------- | :----------------- | :--------------------------------------------- |
| **3B**         | `20 * 3B = 60B` tokens             | **1T** tokens      | **严重过训练** (16.7x 推荐量)，追求极致的模型潜力 |
| **7B**         | `20 * 7B = 140B` tokens            | **1T** tokens      | **深度过训练** (7.1x 推荐量)，当前业界甜点模型的主流策略 |
| **13B**        | `20 * 13B = 260B` tokens           | **1T** tokens      | **显著过训练** (3.8x 推荐量)，在数据和模型规模间取得良好平衡 |

向思考，对于我们固定的 1T token 数据集，Chinchilla 推荐的最优模型大小应为 `N = 1T / 20 = 50B`。这揭示了我们项目的一个核心战略选择：我们**并非追求给定 FLOPs 下的最低 PPL**，而是**追求在给定的中等模型规模（3B/7B/13B）下，通过更充分的数据灌输，来获得更强的泛化能力和下游任务表现**。这将在 2.3 节详细展开。

#### 2.2 噪声尺度定律：大批量训练的动力学与边界

缩放定律不仅宏观地指导 `N` 和 `T` 的配比，也微观地影响着训练过程中的关键超参——**全局批次大小 (Global Batch Size, `GB_tok`)**。简单地将批次大小加倍并期望训练时间减半的线性思维，在实践中会迅速碰壁。其背后的理论支撑就是噪声尺度定律。

在随机梯度下降（SGD）及其变体（如 AdamW）中，我们用一个 mini-batch 计算出的梯度 `g` 来近似整个数据集上的真实梯度 `∇L`。这个 `g` 是一个有噪声的估计。**梯度信噪比 (Gradient Signal-to-Noise Ratio, GSNR)** 是衡量这个估计质量的关键指标。

大批量训练的核心收益在于，通过平均更多样本的梯度，可以有效降低梯度估计的方差，从而提高 GSNR。然而，这种收益并非无限的。噪声尺度定律揭示了两个不同的训练区域：

*   **噪声主导区 (Noise-Dominated Regime)**：当 `GB_tok` 较小时，梯度估计的方差很大，严重干扰了模型的下降方向。在此区域，**增大批次大小几乎可以带来线性的训练加速**。因为每个梯度步都更“准”，我们可以使用更大的学习率，从而在更少的步骤内达到相同的损失。
*   **曲率主导区 (Curvature-Dominated Regime)**：当 `GB_tok` 超过一个**临界批次大小 `B*`** (Critical Batch Size) 后，梯度估计已经非常精确，噪声不再是主要矛盾。此时，训练的瓶颈变为了损失曲面本身的曲率。即使再增大批次，也无法让模型“抄近道”，因为下降路径已经被损失景观的“地形”所决定。在此区域，增大批次大小带来的收益会急剧下降，进入**收益递减**的“饱和区”。

```ascii
      ▲ Throughput Gain (Speedup)
      │
      │       /
      │      /  <-- 线性加速区 (Noise-Dominated)
      │     /
      │    /
 B*   │---/-------------------  <-- 收益饱和区 (Curvature-Dominated)
      │  /
      │ /
      └─────────────────────────► Global Batch Size (GB_tok)
```

**Rule-of-thumb (Noise Scale & Batch Size):**
> 1.  **目标区间**：我们的目标是选择一个 `GB_tok`，使其**足够大以充分利用硬件并行性（高 tokens/s），但又刚好落在临界点 `B*` 附近或略低于它的位置**，以避免进入深度饱和区，造成统计效率的浪费。
> 2.  **实践甜点**：对于 7B/13B 规模的模型在 A100/H100 集群上训练，业界普遍采用的 `GB_tok` 范围是 **2M 到 4M tokens**。例如LLaMA-1 使用了 4M tokens 的全局批次。这个区间通常是在硬件吞吐和统计效率之间权衡的结果。
> 3.  **批次与步数的关系**：选择 `GB_tok` 是一个战略决策。一旦 `GB_tok` 和总训练 tokens `T_tokens` 确定，总训练步数 `iters` 就被锁定了：`iters = T_tokens / GB_tok`。这个 `iters` 直接决定了我们学习率调度器（LR schedule）的整个生命周期，例如 cosine decay 的下降曲线。一个过大的 `GB_tok` 会导致 `iters` 过少，可能使模型在学习率衰减到底部之前还未充分收敛。

#### 2.3 2024 年对 Chinchilla 规律的刷新与实践补充

Chinchilla 定律是 LLM 训练的基石，但它是在 2022 年的认知水平下得出的。随着业界训练了更多、更大、更强的模型，我们对缩放定律的理解也更加深刻和 nuanced。以下是结合 2024 年最新共识的几点关键补充：

##### 2.3.1 “过度训练”的公认价值 (The Proven Value of "Over-training")

Meta 的 LLaMA 列模型是这一理念的旗手。LLaMA-1 7B 用 1T tokens 训练，LLaMA-2 7B 用 2T tokens，都远远超过了 Chinchilla 的 140B token 建议。最新的 Llama 3 8B 据传也使用了超过 15T tokens 进行训练。这一系列的成功雄辩地证明：**Chinchilla 的“计算最优”仅针对预训练损失，而下游任务的泛化能力、推理能力和对齐能力，似乎从更长时间的高质量数据训练中获益匪浅。**

**为什么“过度训练”有效？**
*   **学习稀有模式**：语言中存在大量低频但重要的知识和推理模式。更长的训练使模型有更多机会看到并内化这些“长尾”现象。
*   **形成更鲁棒的“神经回路”**：模型内部用于执行特定任务（如算术、代码生成）的“神经回路”在更长的训练中得到反复锤炼和优化，变得更加稳定和泛化。
*   **更好的正则化**：在海量、多样化的数据上进行长时间训练，本身就是一种强大的正则化，迫使模学习更普适的表征，而不是记忆特定样本。

##### 2.3.2 数据质量乘数效应 (The Data Quality Multiplier)

缩放定律中的 `T` 并非同质的。现代共识认为，数据质量的作用远超数量。微软的 Phi 系列模型用教科书级别的“合成+精选”数据，以小得多的模型和数据量实现了惊人的性能，就是最好的例证。我们可以将缩放定律中的 `T` 想象成 $T_{\text{effective}} = Q \cdot T_{\text{raw}}$，其中 `Q` 是一个难以量化但至关重要的**质量因子**。

**高质量数据的维度：**
*   **多样性 (Diversity)**：覆盖广泛的领域（维基百科、书籍、代码、对话）、文体和语言。
*   **洁净度 (Cleanliness)**：去除格式错误、模板化内容、PII（个人身份信息）和有毒内容。
*   **复杂度 (Complexity)**：包含需要多步推理、深入理解和抽象思维的内容（如高质量代码、数学论文、哲学论述）。
*   **无污染 (Non-Contamination)**：确保用于评估的下游任务数据没有以任何形式泄漏到训练集中，这是保证评估有效性的生命线。

##### 2.3.3 长上下文的新战场 (The Long Context Frontier)

Chinchilla 等早期研究主要基于 2k 左右的上下文。当我们扩展到 4k/8k 甚至更长时，挑战是双重的：
*   **计算挑战**：标准 Transformer 的自注意力计算是 $O(L_{ctx}^2)$ 复杂度。FlashAttention v2 等技术通过 I/O 感知算法和算子融合，极大地优化了实际计算的**墙钟时间**和**显存占用**，但并未改变理论计算量。
*   **统计挑战**：模型需要接触到足够多**真正包含长距离依赖关系**的样本，才能学会利用长上下文。简单地将短文档拼接起来是无效的。这要求数据集中必须有高质量的长篇文档、书籍、代码库等。

目前业界尚无针对长上下文的精确缩放定律，但普遍认为，要让模型有效利用长上下文，可能需要比短上下文模型更多或更专门数据。

##### 2.3.4 多轮次训练与数据课程 (Multi-Epoch Training & Data Curriculum)

传统的大规模学习范式通常建议在海量数据上只训练一个 epoch，以最大化接触新数据的效率。然而，随着对数据质量的重视，新趋势是：**在一个规模有限但极高质量的数据集上进行多轮次（2-4 epochs）训练，可能比在一个庞大但质量参差不齐的数据集上训练一轮效果更好。** 这强化了模型对核心知识的学习。

此外，**数据课程（Data Curriculum）**也重新受到关注。即在训练的不同阶段，动态调整不同数据源的混合比例。例如：
*   **阶段一**：主要使用通用、高质量的语料（如书籍、维基百科）来构建模型的基础语言能力。
*   **阶段二**：逐渐增加代码、科学文献等专业领域数据的比例，以注入专门知识。
*   **阶段三**：在训练后期，可以适当提高对话、指令等数据比例，为后续的对齐微调做准。

### 3. 本章小结

本章系统地阐述了指导 LLM 训练的缩放定律及其最新发展，为我们的项目提供了坚实的理论基础。

*   **核心权衡**：训练的核心决策是在固定的计算预算 `C ≈ 6NT` 下，分配模型参数 `N` 和训练数据 `T`。
*   **Chinchilla 基准**：Chinchilla 定律指出，为实现计算最优（最低预训练损失），应遵循 `T ≈ 20 * N`。这为我们提供了一个重要的理论**参考点**，而非必须遵守的教条。
    $$
    L(N, T) \approx E_{\infty} + \frac{A}{N^{\alpha}} + \frac{B}{T^{\beta}} \quad \text{s.t.} \quad C \approx 6NT
    $$
*   **大批量训练的边界**：噪声尺度定律告诉我们，存在一个**临界批次大小 `B*`**。我们的策略是选择一个足够大以饱和硬件，但又不超过 `B*` 太多的 `GB_tok`（例如 2M-4M tokens），以在硬件效率和统计效率间取得平衡。
*   **现代实践的演进**：
    *   **“过度训练”** 小模型（用远超 Chinchilla 建议的数据量）是提升模型综合能力（特别是泛化和推理）的行业标准策略。
    *   **数据质量**至上，其重要性在缩放定律中的权重日益增加，甚至超过原始数据量。
    *   **长上下文**和**数据课程**是当前 scaling law 研究的前沿，对数据准备提出了新的要求。
    *   我们的 **1T token 训练计划**，正是基于这些现代认知，对 3B/7B/13B 模型进行深度“过度训练”的战略选择。

### 4. 常见陷阱与错误 (Gotchas)

1.  **陷阱：将“计算最优”奉为圭臬**
    *   **症状**：项目负责人坚持要为 1T token 数据集训练一个 50B 模型，因为“Chinchilla 论文是这么说的”，而忽略了实际的部署成本和推理延迟限制。
    *   **诊断**：混淆了“在固定 FLOPs 下 PPL 最低”与“对业务最有价值的模型”。
    *   **处方**：明确训练目标。如果目标是部署一个高性能的 7B 模型，那么就应该用充足的数据（如 1T tokens）去“喂饱”它，而不是去训练一个无法部署的、只是“计算最优”的更大模型。

2.  **陷阱：盲目追求 Token 数量，忽视质量和去重**
    *   **症状**：训练损失曲线在早期迅速下降后很快就进入平台期，或者模型在生成时频繁输出重复、模板化的无意义文本。
    *   **诊断**：训练数据中可能存在大量重复或低质内容。模型快速地学会了这些高频模式并对其过拟合，而泛化能力极差。
    *   **处方**：在数据预处理阶段投入足够的工程和研究资源。实施严格的、多层次的去重策略（从精确匹配到模糊去重）。建立数据质量评估流水线，通过语言模型困惑度、启发式规则等方式过滤掉低价值数据。

3.  **陷阱：在大批量下，天真地进行线性学习率缩放**
    *   **症状**：将全局批次从 1M tokens 增加到 4M tokens，同时将学习率也线性地提高了 4 倍，结果训练立刻出现 loss spike 甚至 NaN。
    *   **诊断**：虽然存在学习率应随批次大小增大的规律（将在下一章详述），但简单的线性缩放（Linear Scaling Rule）在超出临界批次大小 `B*` 后往往会失效。此时梯度噪声减小，信噪比提高，过大的学习率会导致优化器步长过大，直接“飞出”损失函数的稳定区域。
    *   **处方**：采用更温和的缩放策略，如平方根缩放（Sqrt Scaling Rule），或者在增大批次时，仅小幅提高学习率并加强 warmup 阶段的监控。

4.  **陷阱：用预训练的缩放定律来规划微调或 CPT**
    *   **症状**：在一个高质量的 1B token 专业领域数据集上对一个 7B 基座模型进行 CPT，参考 `T=20N` 规则后认为数据量远远不够。
    *   **诊断**：混淆了从零学习和知识注入的动力学。CPT 的起始点是一个已经具备强大先验知识的模型，其学习效率和数据需求与预训练完全不同。
    *   **处方**：CPT/微调的数据量规划更多地依赖于经验和具体任务。其核心在于数据的高质量和与目标领域的强相关性，而非绝对数量。其 scaling law 有自己独立的规律，通常需要的数据量远小于预训练。
