# chapter10.md — 评估：验证困惑度

## 开篇段落

本章是衡量我们训练成果的标尺，也是漫长训练征途中最可靠的指南针。当模型在数以千计的 GPU 上处理着万亿级别的 token 时，我们需要一个稳定、可解释、且具有指导意义的量化指标来回答那个最核心的问题：“模型学得怎么样了？”。对于语言模型而言，这个核心指标就是**困惑度（Perplexity, PPL）**。它看似简单——只是损失函数的一个指数变换，但其背后蕴含着深刻的评估哲学与复杂的实践细节。本章将从 PPL 的信息论根源出发，深入探讨其在长上下文场景下的鲁棒评估方法（滑动窗口）、如何构建无污染且多维度的评估集，以及如何从 PPL 曲线的细微波动中诊断出训练过程中的分布漂移、知识遗忘和数据比失衡等疑难杂症。最终目标是让读者能够建立一套科学、严谨的评估体系，用数据驱动的方式，精确指导模型的每一次迭代与优化。

## 文字论述

### 1. 困惑度 (Perplexity, PPL) 的深度解析

困惑度是信息论在自然语言处理中的直接应用，它量化了模型对一个未知序列的“惊讶程度”。一个优秀的语言模型，如同一个领域的专家，对接下来将要发生的事件（下一个词）应该有较强的预见性，即较低的“惊讶程度”。PPL 越低，代表模型对语言的概率分布建模越好。

#### 1.1 从交叉熵到困惑度

数学上，对于一个 token 序列 $X = (x_1, x_2, \dots, x_T)$，模型的**交叉熵损失 (Cross-Entropy Loss)** 定义为该序列的平均负对数似然 (Negative Log-Likelihood, NLL)：

$$
H(p, q) = \text{Loss}_{\text{CE}} = -\frac{1}{T} \sum_{t=1}^{T} \log p_{\theta}(x_t | x_{<t})
$$

其中：
- $p_{\theta}(x_t | x_{<t})$ 是由参数为 $\theta$ 的模型算出的，在给定上文 $x_{<t}$ 的条件下，真实下一个 token $x_t$ 出现的概率。
- 这个值正是我们在训练中最小化的目标函数。

困惑度 PPL 则是交叉熵的指数形式：

$$
\text{PPL}(X) = \exp(H(p, q)) = \exp\left(-\frac{1}{T} \sum_{t=1}^{T} \log p_{\theta}(x_t | x_{<t})\right)
$$

这个指数关系让 PPL 更具解释性。如果一个模型的 PPL 是 10，可以通俗地理解为，在每个位置进行预测时，模型的不确定性等价于在 10 个可能的选项中进行均匀随机猜测。

**信息论视角**：交叉熵可以被解释为使用模型 $p_{\theta}$ 编码真实数据分布 $q$ 所需的平均比特数。PPL 则是“有效词汇量”，即模型在每个决策点平均面对的分支数量。

#### 1.2 PPL 的归一化与比较基准

PPL 的绝对值高度依赖于词汇表大小和文本的性质（例如，代码的 PPL 通常高于散文）。因此，在比较不同实验的 PPL 时，必须保证评估设置的一致性。
*   **Per-token PPL**：这是 LLM 预训练中最标准的度量方式，如上文公式所示，在 token 层面进行归一化。
*   **Per-word PPL / Per-byte PPL**：在某些研究中，为了跨 Tokenizer 进行比较，会使用词级别或字节级别的 PPL。但这需要复杂的转换，在我们的 LLaMA-style 训练中，统一使用 Per-token PPL 即可。

**Rule-of-Thumb:**
*   对于一个拥有 `V=32000` 词汇表的模型，一个完全随机的猜测会产生 `PPL ≈ 32000`。
*   在高质量英文语料（如 Pile 验证集）上，一个训练良好的 7B-13B 级别模型，在 1T token 训练后，其验证 PPL 通常会收敛到 **2.5-3.5** 的区间。这个数字可以作为训练是否成功的关键 sanity check。低于 2.5 意味着非常优秀，而高于 4.0 则可能表示训练不足或存在问题。

### 2. 长上下文评估：滑动窗口策略 (Sliding Window PPL) 的精细化

当待评估的文档长度（例如，一篇 50k token 的论文）远超模型的最大上文长度 `L_ctx`（例如 8k）时，必须采用滑动窗口策略。这其中有几个关键的技术细节决定了评估的准确性和可复现性。

**精细化操作流程:**
1.  **分块与步长 (Stride)**：将长文档 $D$ 切分为多个重叠的窗口。第一个窗口是 `D[0 : L_ctx]`。后续窗口通过向前滑动一个步长 `stride` 得到，即 `D[i*stride : i*stride + L_ctx]`。
    *   **`stride` 的选择**：`stride` 是计算效率和评估精度的权衡。
        *   小 `stride`（如 128）：评估更精确，因为每个 token 都有机会在接近窗口末端的位置被预测（此时上下文最充分），但计算成本极高。
        *   大 `stride`（如 `L_ctx`）：无重叠，计算最快，但结果偏差最大，因为每个 token 的预测上下文质量不一。
        *   **推荐实践**：`stride = L_ctx / 2` 或 `L_ctx / 4`（如 4096 或 2048 for `L_ctx=8k`）是一个公认的良好折中。
2.  **上下文预热与去偏 (Context Priming & Debiasing)**：窗口开头的 token 由于上文信息不足，其预测概率通常偏低，会不成比例地拉高 PPL。为消除这种边界效应，我们只计算每个窗口中特定部分的损失。
    *   **标准做法**：对于每个窗口，只计算后 `stride` 个 token 的损失。这样，除了第一个窗口外，每个被评估的 token 都至少有 `L_ctx - stride` 长度的“预热”上下文。
3.  **聚合 (Aggregation)**：正确聚合所有窗口的损失至关重要。
    *   **步骤**：累加所有窗口中被计算部分的**总负对数似然 (sum of NLL)**，同时累加被计算的**总 token 数量**。
    *   **最终计算**： `Total_PPL = exp(Total_NLL / Total_Tokens)`。

**ASCII 图示 (L_ctx=8k, stride=4k):**

```
Document: [ t_1, t_2, ..., t_50000 ]

Window 1: [ t_1 ... t_8192 ]
          |--- ignored for loss ---| |--- loss calculated on ---|
          Context: [ t_1...t_4096 ]  Tokens for loss: [ t_4097...t_8192 ]

Window 2: [ t_4097 ... t_12288 ]
          |--- ignored for loss ---| |--- loss calculated on ---|
          Context: [ t_4097...t_8192 ] Tokens for loss: [ t_8193...t_12288 ]

Window 3: [ t_8193 ... t_16384 ]
          |--- ignored for loss ---| |--- loss calculated on ---|
          Context: [ t_8193...t_12288 ] Tokens for loss: [ t_12289...t_16384 ]
...
```

**高级考量：Attention Sinks**
最近的研究（如 StreamingLLM）发现，即使有 RoPE 等位置编码，Transformer 模型也倾向于将大量注意力权重分配给初始的几个 token（所谓的 "Attention Sink"）。在标准的滑动窗口评估中，每个窗口都是独立的，没有这些初始的 "sink tokens"，可能导致 PPL 虚高，无法真实反映模型在流式推理时的性能。
*   **实践建议**：在评估时，可以为每个窗口都加上一个固定的 `BOS` token 或者一小段固定的前缀文本作为 "Attention Sink" token，并确保在计算 loss 时忽略它们。这能让评估结果更接近模型在真实部署场景下的表现。

### 3. 评估集的构建：科学与艺术

评估集的质量直接决定了 PPL 指标的可靠性和指导价值。
*   **严格的 Held-Out 集**：验证集和测试集必须与训练集完全隔离。数据污染是导致 PPL 虚高、误导模型选择的头号杀手。
    *   **去污策略**：在构建验证集后，必须使用工具（如 MinHash LSH, n-gram 重叠检查）来检测并移除与训练集中任何文档有显著重叠的样本。这个过程应被视为数据预处理流程中不可或缺的一环。
*   **多维度、分领域的评估**：单一的整体 PPL 可能会掩盖模型在不同能力维度上的优劣。
    *   **分布内 (ID)**：从与训练集主体同分布的数据中采样，如一个 held-out 的 C4 shard。这反映了模型对核心任务的学习程度。
    *   **近分布/分布外 (Near/Out-of-Distribution, OOD)**：选择不同但相关的领域。例如，如果训练集主要是通用网页文本，那么 GitHub 代码、arXiv 论文摘要、法律文等就是很好的 OOD 评估集。
    *   **评估集大小**：验证集应足够大以保证统计显著性。通常，一个包含数百万到上千万 token 的验证集是比较可靠的，可以在评估时间和结果稳定性之间取得平衡。

### 4. CPT 评估：在“遗忘”与“学习”之间走钢丝

对于继续预训练（CPT），评估的核心是量化模型在**域适应性（domain adaptation）**和**知识保持性（knowledge retention）**之间的权衡。

1.  **建立全面的基线**：在 CPT 开始前，用**基座模型**在一系列精心挑选的验证集上运行评估，得到一组基线 PPL。这些验证集应至少覆盖：
    *   **基座核心领域 (Base Core)**：例如，来自 Pile 的一个子集。
    *   **目标新领域 (Target Domain)**：例如，一个 held-out 的医学文献数据集。
    *   **通用能力探针 (Canary Sets)**：一些小型的、代表特定能力的数据集，如代码、对话、数学推理等。

2.  **持续追 PPL 矩阵**：在 CPT 过程中，定期（例如每 5000 步或在学习率变化的关键节点）在**所有**这些验证集上重新计算 PPL，并观察其动态变化。

**PPL 矩阵动态分析示例：**

| 训练步数 | Base Core PPL | Target Domain PPL | Code Canary PPL |
| :------- | :------------ | :---------------- | :-------------- |
| 0 (Base) | 3.15          | 12.80             | 4.50            |
| 5k steps | 3.20 (+0.05)  | 7.50 (-5.30)      | 4.65 (+0.15)    |
| 10k steps| 3.28 (+0.13)  | 5.10 (-7.70)      | 4.80 (+0.30)    |

**解读**:
*   `Target Domain PPL` 大幅下降，表明域适应非常成功。
*   `Base Core PPL` 和 `Code Canary PPL` 有轻微上升，表明存在一定程度的知识遗忘，但仍在可接受范围内。如果这些指标急剧恶化，就需要调整 CPT 策略（例如，降低学习率、增加基座数据的混比，见 `chapter06`）。

### 5. 从 PPL 曲线诊断训练疑难杂症

PPL 曲线不仅是成绩单，更是训练过程的“心图”或“示波器”。通过监控按数据来源分解的 PPL，我们可以洞察许多深层次问题。

*   **诊断分布漂移 (Distribution Shift)**：如果在训练中途改变了数据混合比例（例如，从通用语料为主切换到代码语料为主），你会看到 `val_general` 的 PPL 下降趋势变缓甚至略微上升，而 `val_code` 的 PPL 开始加速下降。这清晰地反映了模型注意力的转移。

*   **诊断“过拟合”特定数据源**：假设你的混比是 70% 网页、20% 书籍、10% 代码。如果 `ppl_web` 的收敛速度远超其他两者，而你的目标是均衡发展，这便是一个明确的信号：需要通过动态采样（`chapter06`）提高书籍和代码的采样温度或权重。

*   **识别不稳定的学习阶段**：PPL 曲线的剧烈震荡通常与学习率过高或数据质量问题有关。特别是在 large-batch 训练中，如果学习率与 batch size 的 scaling（`chapter05`）不匹配，PPL 曲线会呈现出锯齿，而不是平滑下降。

## 本章小结

*   **困惑度 (PPL)** 是交叉熵损失的指数形式 (`PPL = exp(loss)`)，是衡量语言模型预测能力的核心指标，其值越低代表模型性能越好。
*   长文本评估需采用**滑动窗口**策略，通过设置合理的 `stride`（如 `L_ctx/2`）并仅计算窗口后半部分的损失，来获得稳定且去偏的结果，同时可考虑引入 "Attention Sinks" 提高评估的真实性。
*   评估集必须严格**与训练集分离**（通过去污工具验证），并覆盖分布内 (ID)、分布外 (OOD) 和能力探针 (Canary) 等多个维度，以进行全面评估。
*   **CPT 评估**是一个动态的权衡过程，需要通过监控 PPL 矩阵来量化模型在新领域的学习效果与在旧领域的知识遗忘程度。
*   按数据来源**分解 PPL 曲线**是高级的诊断工具，能够揭示分布漂移、数据配比失衡和训练不稳定性等深层问题，为调整训练策略提供直接依据。

## 常见陷阱与错误 (Gotchas)

1.  **数据污染 (Data Contamination)**：最致命的错误。验证集样本意外出现在训练集中，会导致 PPL 被严重低估，产生“模型效果超常”的假象。**调试技巧**：在数据预处理阶段，强制使用 MinHash 或其他文档指纹技术对训练集和验证集进行交叉去重。
2.  **Tokenizer 不一致**：训练和评估流程必须使用字节级完全相同的 Tokenizer 实例。任何细微差异（如特殊 token 的添加、词表文件版本、正则化规则）都会导致 token ID 错位，使 PPL 结果毫无意义。**调试技巧**：将 tokenizer 配置与模型检查点一同保存和加载，确保评估脚本从唯一的、可信的来源加载 tokenizer。
3.  **平均 PPL 的数学错误**：绝对不能直接对不同数据集的 PPL 值进行平均。PPL 是非线性度量。**正确做法**：分别计算每个数据集的总 NLL 和总 token 数，将它们加总后，再计算整体的 PPL。即 `PPL_total = exp( (NLL_1 + NLL_2) / (Tokens_1 + Tokens_2) )`。
4.  **序列打包 (Packing) 与损失掩码 (Loss Masking)**：为了提高训练吞吐，常将多个短文档拼接成一个长序列。在计算 PPL 时，必须使用正确的损失掩码，确保模型不会试图预测一个文档的结尾到下一个文档的开头。**调试技巧**：可视化损失掩码，确保在文档边界处的 token 损失被置零。一个错误的掩码可能导致 PPL 被人为拉低（如果边界被预测得很好）或拉高。
5.  **BOS/EOS Token 处理不一致**：是否计算对第一个 token (BOS) 的预测损失，以及如何处理序列末尾的 EOS token，必须在所有实验和比较中保持严格一致。**行业惯例**：通常不计算对 BOS token 的预测损失，因为其没有上文。
6.  **评估效率瓶颈**：在大型训练任务中，一次完整的验证集评估可能耗时数小时。如果评估过于频繁或效率低下，会严重拖慢整个迭代周期。**优化技巧**：在 PyTorch Lightning 中，评估在独立的 `validation_step` 中进行，确保开启 `torch.no_grad()`，使用尽可能大的评估 batch size（因为没有梯度和优化器状态的显存开销），并利用 DDP/FSDP 并行处理数据分片。
7.  **忽略 PPL 的对数尺度**：PPL 的改进是指数性的。PPL 从 20 降到 10 的难度远小于从 3.1 降到 3.0。在训练后期，不要因为 PPL 的微小波动而过度反应，应关注其在数千步尺度上的平滑移动平均趋势。
